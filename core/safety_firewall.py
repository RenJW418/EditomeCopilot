"""
Safety Firewall â€” Hallucination Guard & Contradiction Detection
===============================================================
Inspired by: MedGraphRAG (Wu et al. 2024) safety mechanism,
             Cancer Cell RAG-LLM (Jun et al. 2026) precision medicine safety.

Purpose:
1. Fact-check every generated response against KG + Almanac
2. Detect contradictions with authoritative sources
3. Quantify uncertainty (confidence scoring)
4. Enforce red-line rules for gene editing safety (irreversible DNA edits)
"""

from __future__ import annotations

import re
import json
from typing import Any, Dict, List, Optional, Tuple


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Red-Line Rules â€” MUST NOT be violated in any response
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RED_LINE_RULES = [
    {
        "id": "RL-001",
        "rule": "ç¦æ­¢æ¨èäººç±»ç”Ÿæ®–ç³»ç¼–è¾‘ (germline editing) ç”¨äºä¸´åºŠåº”ç”¨",
        "trigger_keywords": ["germline editing", "ç”Ÿæ®–ç³»ç¼–è¾‘", "heritable", "embryo editing",
                             "èƒšèƒç¼–è¾‘", "germline therapy"],
        "exception": "å¯ä»¥è®¨è®ºå·²å‘è¡¨çš„åŸºç¡€ç ”ç©¶, ä½†å¿…é¡»æ˜ç¡®æ ‡æ³¨: 'æ­¤ä¸ºåŸºç¡€ç ”ç©¶, ä¸å»ºè®®ä¸´åºŠè½¬åŒ–'",
        "severity": "CRITICAL",
    },
    {
        "id": "RL-002",
        "rule": "ç¦æ­¢æ¨èenhancementç”¨é€”çš„åŸºå› ç¼–è¾‘",
        "trigger_keywords": ["enhancement", "å¢å¼º", "designer baby", "cognitive enhancement",
                             "genetic enhancement"],
        "exception": "ä»…å¯åœ¨ä¼¦ç†è®¨è®ºè¯­å¢ƒä¸‹æåŠ",
        "severity": "CRITICAL",
    },
    {
        "id": "RL-003",
        "rule": "ä¸å¾—å°†ä¸´åºŠå‰æ•°æ®(pre-clinical)å‘ˆç°ä¸ºå·²è·æ‰¹ç–—æ³•",
        "trigger_keywords": [],
        "check_type": "evidence_level_mismatch",
        "severity": "HIGH",
    },
    {
        "id": "RL-004",
        "rule": "å¿…é¡»æŠ«éœ²off-targeté£é™©: ä»»ä½•æ¶‰åŠCRISPR/Base/Prime editingçš„æ²»ç–—å»ºè®®å¿…é¡»æåŠè„±é¶é£é™©",
        "trigger_keywords": [],
        "check_type": "missing_safety_disclosure",
        "severity": "HIGH",
    },
    {
        "id": "RL-005",
        "rule": "ä¸å¾—æ¨èæœªéªŒè¯çš„guide RNAç”¨äºäººç±»ç»†èƒå®éªŒ",
        "trigger_keywords": ["æ¨èsgRNA", "æ¨èguide", "use this gRNA for patient",
                             "recommend this guide"],
        "exception": "å¯ä»¥å±•ç¤ºæ–‡çŒ®ä¸­éªŒè¯è¿‡çš„guide RNA, æ ‡æ³¨æ¥æº",
        "severity": "HIGH",
    },
    {
        "id": "RL-006",
        "rule": "ä¸å¾—å£°ç§°100%ç¼–è¾‘æ•ˆç‡æˆ–é›¶è„±é¶",
        "trigger_keywords": ["100% efficiency", "100%æ•ˆç‡", "zero off-target",
                             "no off-target", "é›¶è„±é¶", "å®Œå…¨æ— è„±é¶"],
        "severity": "HIGH",
    },
]


class SafetyFirewall:
    """Post-generation safety checker for gene-editing RAG system."""

    def __init__(self, llm_client=None, almanac=None, knowledge_graph=None):
        """
        Parameters
        ----------
        llm_client : LLMClient, optional
            For LLM-based contradiction detection.
        almanac : GEAlmanac, optional
            For fact-checking against structured data.
        knowledge_graph : KnowledgeGraph or TripleGraph, optional
            For KG-based consistency checking.
        """
        self.llm = llm_client
        self.almanac = almanac
        self.kg = knowledge_graph

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. Red-Line Check (fast, rule-based)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def check_red_lines(self, response: str) -> List[Dict[str, Any]]:
        """
        Check response against red-line rules.

        Returns list of violations. Empty list = all clear.
        """
        violations = []
        response_lower = response.lower()

        for rule in RED_LINE_RULES:
            # Keyword-based rules
            triggered_keywords = [
                kw for kw in rule.get("trigger_keywords", [])
                if kw.lower() in response_lower
            ]
            if triggered_keywords:
                violations.append({
                    "rule_id": rule["id"],
                    "rule": rule["rule"],
                    "severity": rule["severity"],
                    "triggered_by": triggered_keywords,
                    "exception": rule.get("exception", ""),
                })

        return violations

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. Contradiction Detection (LLM-based)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def detect_contradictions(
        self,
        response: str,
        provenance: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Use LLM to detect contradictions between the response and
        retrieved evidence.

        Returns
        -------
        dict with contradictions list and consistency_score.
        """
        if not self.llm or not hasattr(self.llm, "generate"):
            return {"contradictions": [], "consistency_score": -1,
                    "error": "No LLM available"}

        evidence_text = "\n".join(
            f"[{i+1}] {p.get('title', 'N/A')}: "
            f"{p.get('evidence', p.get('text', p.get('abstract', '')))[:300]}"
            for i, p in enumerate(provenance[:8])
        )

        prompt = (
            "ä½ æ˜¯ä¸€ä¸ªåŸºå› ç¼–è¾‘é¢†åŸŸçš„äº‹å®æ ¸æŸ¥å‘˜ã€‚\n"
            "è¯·å¯¹æ¯”ä»¥ä¸‹ [å›ç­”] å’Œ [è¯æ®], æ‰¾å‡ºæ‰€æœ‰çŸ›ç›¾ä¹‹å¤„ã€‚\n\n"
            "çŸ›ç›¾ç±»å‹:\n"
            "1. æ•°æ®çŸ›ç›¾ â€” å›ç­”ä¸­çš„æ•°å­—(æ•ˆç‡ã€è„±é¶ç‡ç­‰)ä¸è¯æ®ä¸ç¬¦\n"
            "2. å…³ç³»çŸ›ç›¾ â€” å›ç­”ä¸­çš„å› æœ/é€‚ç”¨å…³ç³»ä¸è¯æ®ç›¸å\n"
            "3. æ—¶é—´çŸ›ç›¾ â€” å›ç­”å£°ç§°çš„æ—¶é—´çº¿ä¸è¯æ®ä¸ç¬¦\n"
            "4. è¿‡åº¦å£°æ˜ â€” å›ç­”ä¸­çš„ç¡®å®šæ€§è¯­æ°”è¶…å‡ºè¯æ®æ”¯æŒèŒƒå›´\n\n"
            f"## å›ç­”\n{response[:2000]}\n\n"
            f"## è¯æ®\n{evidence_text}\n\n"
            'è¯·è¾“å‡ºJSONæ•°ç»„, æ¯ä¸ªçŸ›ç›¾ä¸ºä¸€ä¸ªå¯¹è±¡:\n'
            '[{"type": "æ•°æ®çŸ›ç›¾|å…³ç³»çŸ›ç›¾|æ—¶é—´çŸ›ç›¾|è¿‡åº¦å£°æ˜", '
            '"claim_in_answer": "...", '
            '"evidence_says": "...", '
            '"severity": "HIGH|MEDIUM|LOW"}]\n'
            'å¦‚æœæ²¡æœ‰çŸ›ç›¾, è¾“å‡ºç©ºæ•°ç»„ []'
        )

        result = self.llm.generate(prompt, timeout=45, max_tokens=2048)
        if result.startswith("Error"):
            return {"contradictions": [], "consistency_score": -1, "error": result}

        try:
            json_match = re.search(r'\[.*\]', result, re.DOTALL)
            contradictions = json.loads(json_match.group()) if json_match else []
        except (json.JSONDecodeError, AttributeError):
            contradictions = []

        # Score
        severity_weights = {"HIGH": 0.3, "MEDIUM": 0.15, "LOW": 0.05}
        penalty = sum(
            severity_weights.get(c.get("severity", "LOW"), 0.05)
            for c in contradictions
        )
        consistency_score = max(0.0, 1.0 - penalty)

        return {
            "contradictions": contradictions,
            "consistency_score": round(consistency_score, 3),
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. Almanac Fact-Check (structured verification)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def check_against_almanac(self, response: str) -> List[Dict[str, Any]]:
        """
        Check specific claims against GEAlmanac structured data.
        Fast, deterministic, no LLM needed.
        """
        warnings = []
        if not self.almanac:
            return warnings

        resp_lower = response.lower()

        # Check: Does response claim something is FDA-approved that isn't?
        approved_drugs = {a["drug"].lower() for a in self.almanac.approvals}
        approval_patterns = re.findall(
            r'(fda[å·²\s]*(approved|æ‰¹å‡†|è·æ‰¹)|è·å¾—.{0,10}æ‰¹å‡†)',
            resp_lower,
        )
        if approval_patterns:
            # Verify the drug/technology mentioned is actually approved
            for trial in self.almanac.clinical_trials:
                if trial["name"].lower() in resp_lower:
                    if "approved" not in trial.get("status", "").lower():
                        warnings.append({
                            "type": "false_approval_claim",
                            "entity": trial["name"],
                            "actual_status": trial["status"],
                            "severity": "HIGH",
                        })

        # Check: Efficiency claims within known bounds
        efficiency_claims = re.findall(r'(\d{1,3})%\s*(editing|efficiency|æ•ˆç‡|ç¼–è¾‘)', resp_lower)
        for pct_str, _ in efficiency_claims:
            pct = int(pct_str)
            if pct > 99:
                warnings.append({
                    "type": "unrealistic_efficiency",
                    "claim": f"{pct}% efficiency",
                    "context": "No gene editing technology achieves 100% efficiency in vivo",
                    "severity": "HIGH",
                })

        return warnings

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. Uncertainty Quantification
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def quantify_uncertainty(
        self,
        response: str,
        provenance: List[Dict[str, Any]],
        faithfulness_score: float = -1,
    ) -> Dict[str, Any]:
        """
        Estimate confidence based on multiple signals.

        Returns dict with confidence_level, confidence_score, flags.
        """
        flags = []
        score = 1.0

        # Signal 1: Provenance quality
        if not provenance:
            score -= 0.4
            flags.append("No provenance sources found")
        else:
            avg_score = sum(p.get("score", 0) for p in provenance) / len(provenance)
            if avg_score < 0.3:
                score -= 0.2
                flags.append(f"Low average retrieval score: {avg_score:.3f}")

            # Source diversity
            years = [p.get("year", p.get("pub_year", 0)) for p in provenance if p.get("year") or p.get("pub_year")]
            if years:
                latest = max(int(y) for y in years if str(y).isdigit())
                if latest < 2022:
                    score -= 0.1
                    flags.append(f"Most recent source is from {latest} â€” may be outdated")

        # Signal 2: Faithfulness (from evidence chain)
        if faithfulness_score >= 0:
            if faithfulness_score < 0.5:
                score -= 0.3
                flags.append(f"Low faithfulness score: {faithfulness_score}")
            elif faithfulness_score < 0.8:
                score -= 0.1
                flags.append(f"Moderate faithfulness: {faithfulness_score}")

        # Signal 3: Hedging language detection
        hedge_patterns = [
            r'å¯èƒ½|æˆ–è®¸|ä¹Ÿè®¸|ä¸ç¡®å®š|æœ‰å¾…éªŒè¯|å°šæœªè¯å®|åˆæ­¥ç ”ç©¶|éœ€è¦æ›´å¤šç ”ç©¶',
            r'may|might|potentially|uncertain|preliminary|further research needed',
        ]
        hedge_count = sum(len(re.findall(p, response, re.I)) for p in hedge_patterns)
        if hedge_count > 5:
            score -= 0.05
            flags.append(f"High hedging language ({hedge_count} instances)")

        # Signal 4: Response length vs. evidence
        if len(response) > 3000 and len(provenance) < 3:
            score -= 0.1
            flags.append("Long response with few sources â€” possible over-generation")

        score = max(0.0, min(1.0, score))

        # Classify
        if score >= 0.8:
            level = "HIGH"
        elif score >= 0.5:
            level = "MEDIUM"
        else:
            level = "LOW"

        return {
            "confidence_level": level,
            "confidence_score": round(score, 3),
            "flags": flags,
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. Full Safety Pipeline
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run_safety_check(
        self,
        response: str,
        provenance: List[Dict[str, Any]],
        faithfulness_score: float = -1,
    ) -> Dict[str, Any]:
        """
        Run the full safety pipeline:
        1. Red-line violations
        2. Almanac fact-check
        3. LLM contradiction detection
        4. Uncertainty quantification
        5. Final verdict

        Returns
        -------
        dict with all sub-results and a final safety_verdict.
        """
        # 1. Red lines (fast)
        red_line_violations = self.check_red_lines(response)

        # 2. Almanac check (fast)
        almanac_warnings = self.check_against_almanac(response)

        # 3. Contradiction detection (LLM, slower)
        contradiction_result = self.detect_contradictions(response, provenance)

        # 4. Uncertainty
        uncertainty = self.quantify_uncertainty(
            response, provenance, faithfulness_score
        )

        # 5. Final verdict
        critical_violations = [v for v in red_line_violations if v["severity"] == "CRITICAL"]
        high_issues = (
            [v for v in red_line_violations if v["severity"] == "HIGH"] +
            [w for w in almanac_warnings if w.get("severity") == "HIGH"] +
            [c for c in contradiction_result.get("contradictions", []) if c.get("severity") == "HIGH"]
        )

        if critical_violations:
            verdict = "BLOCKED"
            verdict_reason = f"Critical red-line violation(s): {', '.join(v['rule_id'] for v in critical_violations)}"
        elif len(high_issues) >= 2:
            verdict = "FLAGGED"
            verdict_reason = f"{len(high_issues)} high-severity issues detected"
        elif uncertainty["confidence_level"] == "LOW":
            verdict = "LOW_CONFIDENCE"
            verdict_reason = "Insufficient evidence support"
        else:
            verdict = "PASSED"
            verdict_reason = "All checks passed"

        return {
            "safety_verdict": verdict,
            "verdict_reason": verdict_reason,
            "red_line_violations": red_line_violations,
            "almanac_warnings": almanac_warnings,
            "contradictions": contradiction_result,
            "uncertainty": uncertainty,
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 6. Format Safety Report for Display
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def format_safety_report(self, safety_result: Dict[str, Any]) -> str:
        """Format safety check result as Markdown for the user."""
        verdict = safety_result.get("safety_verdict", "UNKNOWN")
        verdict_emoji = {
            "PASSED": "âœ…", "LOW_CONFIDENCE": "âš ï¸",
            "FLAGGED": "ğŸš¨", "BLOCKED": "ğŸ›‘",
        }.get(verdict, "â“")

        lines = [f"### {verdict_emoji} Safety Check: **{verdict}**"]
        lines.append(f"*{safety_result.get('verdict_reason', '')}*\n")

        # Red-line violations
        violations = safety_result.get("red_line_violations", [])
        if violations:
            lines.append("**Red-Line Violations:**")
            for v in violations:
                lines.append(f"- ğŸ›‘ [{v['rule_id']}] {v['rule']}")
                if v.get("exception"):
                    lines.append(f"  *Exception: {v['exception']}*")

        # Almanac warnings
        warnings = safety_result.get("almanac_warnings", [])
        if warnings:
            lines.append("\n**Fact-Check Warnings:**")
            for w in warnings:
                lines.append(f"- âš ï¸ {w['type']}: {w.get('entity', '')} â€” actual: {w.get('actual_status', w.get('context', ''))}")

        # Contradictions
        contradictions = safety_result.get("contradictions", {}).get("contradictions", [])
        if contradictions:
            lines.append("\n**Contradictions Detected:**")
            for c in contradictions:
                lines.append(f"- [{c.get('severity', '?')}] {c.get('type', 'Unknown')}")
                lines.append(f"  Answer claims: {c.get('claim_in_answer', '?')}")
                lines.append(f"  Evidence says: {c.get('evidence_says', '?')}")

        # Confidence
        unc = safety_result.get("uncertainty", {})
        conf = unc.get("confidence_score", -1)
        if conf >= 0:
            lines.append(f"\n**Confidence Score:** {conf:.1%} ({unc.get('confidence_level', '')})")
            for flag in unc.get("flags", []):
                lines.append(f"- {flag}")

        return "\n".join(lines)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 7. Inject Safety Disclaimer
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def inject_disclaimer(
        response: str, safety_result: Dict[str, Any], language: str = "zh"
    ) -> str:
        """
        If safety check flagged issues, prepend a disclaimer banner
        to the response.
        """
        verdict = safety_result.get("safety_verdict", "PASSED")
        if verdict == "PASSED":
            return response

        if verdict == "BLOCKED":
            if language == "zh":
                disclaimer = (
                    "âš ï¸ **å®‰å…¨è­¦å‘Š**: æ­¤å›ç­”è§¦å‘äº†å…³é”®å®‰å…¨è§„åˆ™ï¼Œéƒ¨åˆ†å†…å®¹å¯èƒ½ä¸é€‚åˆç›´æ¥ä½¿ç”¨ã€‚"
                    "è¯·å’¨è¯¢ä¸“ä¸šäººå‘˜ã€‚\n\n"
                )
            else:
                disclaimer = (
                    "âš ï¸ **Safety Warning**: This response triggered critical safety rules "
                    "and may not be suitable for direct use. Please consult experts.\n\n"
                )
        elif verdict == "FLAGGED":
            if language == "zh":
                disclaimer = (
                    "âš ï¸ **æ³¨æ„**: æœ¬å›ç­”ä¸­éƒ¨åˆ†å£°æ˜ä¸å·²çŸ¥è¯æ®å­˜åœ¨ä¸ä¸€è‡´ï¼Œå·²æ ‡æ³¨ã€‚"
                    "è¯·äº¤å‰éªŒè¯åä½¿ç”¨ã€‚\n\n"
                )
            else:
                disclaimer = (
                    "âš ï¸ **Note**: Some claims in this response may be inconsistent with "
                    "known evidence. Please cross-check before use.\n\n"
                )
        else:  # LOW_CONFIDENCE
            if language == "zh":
                disclaimer = "â„¹ï¸ *æœ¬å›ç­”çš„è¯æ®æ”¯æŒåº¦è¾ƒä½ï¼Œè¯·è°¨æ…å‚è€ƒã€‚*\n\n"
            else:
                disclaimer = "â„¹ï¸ *This response has limited evidence support. Use with caution.*\n\n"

        return disclaimer + response
